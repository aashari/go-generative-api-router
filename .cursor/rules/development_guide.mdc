---
description: 
globs: 
alwaysApply: true
---
# Generative API Router Development Guide

This guide provides development workflow, best practices, and architectural guidance for the Generative API Router project.

> **Quick Start**: For setup and running instructions, see [Running and Testing Guide](mdc:running_and_testing.mdc)  
> **Codex Integration**: For AI-assisted development, see [Codex Guide](mdc:codex.mdc)

## üìö **Documentation Quick Reference**

### **Core Development Guides** (.cursor/rules/)
- **[Development Guide](mdc:development_guide.mdc)** - Complete workflow, Git practices, architecture principles (this document)
- **[Running & Testing Guide](mdc:running_and_testing.mdc)** - Setup, testing procedures, debugging techniques
- **[Codex Guide](mdc:codex.mdc)** - AI-assisted development workflow

### **User & API Documentation** (docs/)
- **[User Guide](mdc:../docs/user/README.md)** - *When integrating with the service* - API usage, configuration, client examples
- **[API Reference](mdc:../docs/api)** - *When implementing API calls* - OpenAPI specs, endpoint documentation
- **[Examples](mdc:../examples)** - *When writing client code* - Ready-to-use examples in Python, Node.js, Go

### **Development Documentation** (docs/development/)
- **[Development Guide](mdc:../docs/development/DEVELOPMENT.md)** - *When setting up locally* - Quick start, project structure, daily commands
- **[Contributing Guide](mdc:../docs/development/CONTRIBUTING.md)** - *When contributing code* - PR process, coding standards, review guidelines
- **[Testing Guide](mdc:../docs/development/TESTING.md)** - *When writing/running tests* - Test strategies, coverage, debugging
- **[Deployment Guide](mdc:../docs/development/DEPLOYMENT.md)** - *When managing AWS infrastructure* - xyz-aduh-genapi service, monitoring, troubleshooting

### **Project Overview**
- **[Main README](mdc:../README.md)** - *When getting project overview* - Features, quick start, architecture summary
- **[Documentation Index](mdc:../docs/README.md)** - *When navigating docs* - Complete documentation roadmap

---

## Table of Contents

- [Essential Development Rules](mdc:#essential-development-rules) - Core principles and mandatory approaches
- [Environment Management](mdc:#environment-management) - AWS profiles, variables, and configuration
- [Project Structure](mdc:#project-structure) - Architecture and file organization
- [Terminal Command Rules](mdc:#terminal-command-rules) - Essential command-line best practices
- [Comprehensive Logging](mdc:#comprehensive-logging) - Complete logging without redaction or truncation
- [Development Workflow](mdc:#development-workflow) - Core development cycle with sequential thinking
- [Security Best Practices](mdc:#security-best-practices) - Sensitive data management
- [Git Workflow](mdc:#git-workflow) - PR creation and CI/CD monitoring
- [Release Process](mdc:#release-process) - Version management and releases
- [AWS Deployment & Monitoring](mdc:#aws-deployment--monitoring) - Enhanced ECS deployment and monitoring
- [Maintenance](mdc:#maintenance) - Ongoing maintenance tasks
- [Common Pitfalls](mdc:#common-pitfalls--lessons-learned) - Lessons learned and troubleshooting

## Essential Development Rules

### üö® MANDATORY APPROACH

1. **Sequential Thinking**: Break down complex requests systematically using structured analysis
2. **Research First**: Always investigate existing code, patterns, and conventions before implementing
3. **Commit Discipline**: Only commit and push when explicitly requested by the user
4. **Complete Logging**: Log full data without truncation, cherry-picking, or redaction (see Comprehensive Logging section)
5. **Environment Variables**: Always use proper environment variables for AWS profiles, regions, and configurations
6. **Monitor Logs**: Always check `logs/server.log` during development and debugging
7. **Build Verification**: Run full CI checks locally before any commit

### üîÑ Standard Development Workflow

```bash
# 1. Research and analyze the request
# 2. Make code changes using sequential thinking
# 3. Check logs for issues
tail -20 logs/server.log | grep -E "(error|Error|failed|Failed)" | cat
# 4. Run full verification
make ci-check
# 5. Only commit when user explicitly requests it
```

## Environment Management

### üîß Key Environment Variables

**AWS Configuration:**
```bash
# AWS Profiles and Regions
AWS_PROFILE=your-aws-profile
AWS_REGION=ap-southeast-3

# ECS Clusters and Services
AWS_CLUSTER_DEV=dev-generative-api-router
AWS_SERVICE_DEV=dev-generative-api-router
AWS_CLUSTER_PROD=prod-generative-api-router
AWS_SERVICE_PROD=prod-generative-api-router

# Application Configuration
GENAPI_API_KEY=your-api-key
PORT=8082
LOG_LEVEL=info
```

**Environment Setup:**
```bash
# Load environment variables
export $(cat .env | grep -v '^#' | xargs) && echo "Environment loaded" | cat

# Verify AWS configuration
echo "AWS Profile: $AWS_PROFILE" && echo "AWS Region: $AWS_REGION" | cat

# Verify application configuration
echo "Port: $PORT" && echo "Log Level: $LOG_LEVEL" | cat
```

**AWS Command Patterns:**
```bash
# ECS service status
aws --profile $AWS_PROFILE --region $AWS_REGION ecs describe-services --cluster $AWS_CLUSTER_DEV --services $AWS_SERVICE_DEV | cat

# CloudWatch logs
aws --profile $AWS_PROFILE --region $AWS_REGION logs describe-log-groups --log-group-name-prefix "/ecs/generative-api-router" | cat

# Task definitions
aws --profile $AWS_PROFILE --region $AWS_REGION ecs describe-task-definition --task-definition $AWS_SERVICE_DEV | cat
```

## Project Structure

### Core Application Code
- **`cmd/server/main.go`**: Main application entry point. Initializes and starts the HTTP server. ([cmd/server/main.go](mdc:generative-api-router/generative-api-router/generative-api-router/cmd/server/main.go))
- **`internal/app/app.go`**: Centralizes application configuration and dependencies (reduced to 64 lines after Phase 2 refactoring). ([internal/app/app.go](mdc:generative-api-router/generative-api-router/generative-api-router/internal/app/app.go))
- **`internal/proxy/proxy.go`**: Handles incoming requests, orchestrates vendor selection, request modification, and API communication. ([internal/proxy/proxy.go](mdc:generative-api-router/generative-api-router/generative-api-router/internal/proxy/proxy.go))
- **`internal/proxy/client.go`**: Manages communication with downstream vendor APIs. ([internal/proxy/client.go](mdc:generative-api-router/generative-api-router/generative-api-router/internal/proxy/client.go))
- **`internal/proxy/response_processor.go`**: Processes non-streaming responses from vendors. ([internal/proxy/response_processor.go](mdc:generative-api-router/generative-api-router/generative-api-router/internal/proxy/response_processor.go))
- **`internal/proxy/stream_processor.go`**: Handles streaming responses from vendors. ([internal/proxy/stream_processor.go](mdc:generative-api-router/generative-api-router/generative-api-router/internal/proxy/stream_processor.go))
- **`internal/selector/selector.go`**: Implements strategies for selecting vendors and models. ([internal/selector/selector.go](mdc:generative-api-router/generative-api-router/generative-api-router/internal/selector/selector.go))
- **`internal/validator/validator.go`**: Handles validation of incoming requests. It also extracts the original model name from the request. ([internal/validator/validator.go](mdc:generative-api-router/generative-api-router/generative-api-router/internal/validator/validator.go))
- **`internal/config/config.go`**: Loads and manages configuration from JSON files. ([internal/config/config.go](mdc:generative-api-router/generative-api-router/generative-api-router/internal/config/config.go))
- **`internal/config/validation.go`**: Validates credentials and model configurations. ([internal/config/validation.go](mdc:generative-api-router/generative-api-router/generative-api-router/internal/config/validation.go))
- **`internal/handlers/api_handlers.go`**: HTTP handlers for health, chat completions, and models endpoints. ([internal/handlers/api_handlers.go](mdc:generative-api-router/generative-api-router/generative-api-router/internal/handlers/api_handlers.go))
- **`internal/errors/errors.go`**: Standardized error types and JSON error responses. ([internal/errors/errors.go](mdc:generative-api-router/generative-api-router/generative-api-router/internal/errors/errors.go))
- **`internal/filter/utils.go`**: Utility functions for filtering credentials and models by vendor. ([internal/filter/utils.go](mdc:generative-api-router/generative-api-router/generative-api-router/internal/filter/utils.go))
- **`internal/monitoring/metrics.go`**: Performance profiling endpoints (pprof). ([internal/monitoring/metrics.go](mdc:generative-api-router/generative-api-router/generative-api-router/internal/monitoring/metrics.go))
- **`internal/router/routes.go`**: Centralized route setup with middleware integration. ([internal/router/routes.go](mdc:generative-api-router/generative-api-router/generative-api-router/internal/router/routes.go))

### Configuration Files
- **`configs/credentials.json`**: Stores API keys for different vendors (gitignored). ([configs/credentials.json.example](mdc:generative-api-router/generative-api-router/generative-api-router/configs/credentials.json.example))
- **`configs/models.json`**: Defines the available models and their vendors. ([configs/models.json](mdc:generative-api-router/generative-api-router/generative-api-router/configs/models.json))

### Deployment Files
- **`deployments/docker/Dockerfile`**: For building the Docker image. ([deployments/docker/Dockerfile](mdc:generative-api-router/generative-api-router/generative-api-router/deployments/docker/Dockerfile))
- **`deployments/docker/docker-compose.yml`**: For running the service with Docker Compose. ([deployments/docker/docker-compose.yml](mdc:generative-api-router/generative-api-router/generative-api-router/deployments/docker/docker-compose.yml))
- **`scripts/deploy.sh`**: AWS ECS deployment script (contains sensitive data, gitignored). ([scripts/deploy.sh](mdc:generative-api-router/generative-api-router/generative-api-router/scripts/deploy.sh))

### Development Tools
- **`Makefile`**: Build automation and common tasks. ([Makefile](mdc:generative-api-router/generative-api-router/generative-api-router/Makefile))
- **`scripts/`**: Helper scripts for deployment, testing, and setup
- **`examples/`**: Usage examples for cURL and various client languages
- **`docs/`**: Comprehensive documentation (API, development, user guides)
- **`.golangci.yml`**: Linter configuration for code quality

## Terminal Command Rules

### üö® CRITICAL CONSTRAINTS

When using terminal commands in development, follow these essential rules:

- **SINGLE-LINE ONLY**: No newline characters (`\n`) in commands
- **ALWAYS PIPE TO CAT**: End every command with `| cat` to avoid terminal issues
- **PROPER ESCAPING**: Use single quotes inside double quotes
- **NO LINE CONTINUATION**: Use semicolons (`;`) instead of backslashes (`\`)
- **COMMAND CHAINING**: Use `&&` for conditional execution, `;` for sequential execution

### ‚úÖ Correct Terminal Patterns

```bash
# Git commits with multiple messages
git commit -m "feat: main commit message" -m "- First bullet point" -m "- Second bullet point" | cat

# Command chaining with proper error handling
make build && ./build/server > logs/server.log 2>&1 & echo "Server started" | cat

# Process checking and cleanup
pgrep -f "build/server$" | xargs -r kill -9 && echo "Server stopped" | cat

# Log monitoring with grep
tail -20 logs/server.log | grep -E "(error|Error|failed|Failed)" | cat

# AWS commands with proper formatting
aws --profile $AWS_PROFILE --region $AWS_REGION ecs describe-services --cluster $AWS_CLUSTER --services $AWS_SERVICE | cat
```

### ‚ùå Common Terminal Mistakes to Avoid

```bash
# These patterns will fail - avoid them
echo "multiline \
command"                         # Line continuation doesn't work

command_without_pipe             # Missing | cat can cause issues

curl http://example.com          # Should be: curl http://example.com | cat
```

## Comprehensive Logging

### üéØ Core Logging Principles

The Generative API Router implements comprehensive, unfiltered logging that captures complete data structures without any redaction, truncation, or selective filtering.

**FUNDAMENTAL RULE**: Log everything. External logging systems handle redaction, size management, and sensitive data filtering.

### ‚úÖ Logging Best Practices

1. **Log Complete Data**: Always log entire objects, arrays, and strings without truncation
2. **No Cherry-Picking**: Log complete data structures, not selected attributes
3. **No Redaction**: Log everything including sensitive data (API keys, credentials)
4. **No Truncation**: Never use substring(), slice(), or size limits
5. **No Derived Data**: Don't log .length, .size, .count - log the actual data
6. **Complete Context**: Include all relevant objects, configurations, and state
7. **Raw Data Logging**: Log raw objects without transformations or filtering

### üìù Logging Examples

**‚úÖ Correct Comprehensive Logging:**
```go
// Log complete request data
logger.LogRequest(ctx, "Processing chat completion request", map[string]any{
    "complete_request": request,           // Entire request object
    "headers": headers,                    // All headers including auth
    "vendor_config": vendorConfig,         // Complete vendor configuration
    "api_keys": credentials,               // Complete credentials including keys
    "processing_context": processingCtx,   // Complete processing context
})

// Log complete vendor communication
logger.LogVendorCommunication(ctx, "Sending request to vendor", map[string]any{
    "vendor": vendor,
    "complete_request_payload": payload,   // Complete payload sent to vendor
    "complete_headers": requestHeaders,    // All headers sent
    "api_key": apiKey,                    // Complete API key
    "endpoint_url": endpointURL,          // Complete endpoint
    "timeout_config": timeoutConfig,       // Complete timeout configuration
})

// Log complete response processing
logger.LogResponse(ctx, "Processing vendor response", map[string]any{
    "vendor": vendor,
    "complete_response": response,         // Entire response object
    "response_headers": responseHeaders,   // All response headers
    "original_model": originalModel,       // Original requested model
    "actual_model": actualModel,          // Actual vendor model used
    "processing_metadata": metadata,       // Complete processing metadata
})
```

**‚ùå Avoid Partial/Filtered Logging:**
```go
// Don't do this - partial data logging
logger.Info("Processing request", map[string]any{
    "model": request.Model,              // Only one field
    "message_count": len(request.Messages), // Derived data instead of actual messages
    "api_key_prefix": apiKey[:8] + "...", // Truncated sensitive data
})

// Don't do this - filtered/redacted logging
logger.Info("Vendor response", map[string]any{
    "status": "success",                 // Derived status instead of complete response
    "response_size": len(response),      // Size instead of actual response
    "model": response.Model,             // Single field instead of complete object
})
```

### üîç Log Monitoring Commands

**Essential Log Monitoring:**
```bash
# Monitor real-time logs during development
tail -f logs/server.log

# Check recent logs after testing
tail -50 logs/server.log | cat

# Search for errors
grep -i "error" logs/server.log | tail -10 | cat

# Search for specific components
grep "ProxyHandler" logs/server.log | tail -5 | cat
grep "VendorSelector" logs/server.log | tail -5 | cat

# Monitor specific request flows
grep "request_id.*abc123" logs/server.log | cat

# Check vendor communication logs
grep "VendorCommunication" logs/server.log | tail -10 | cat
```

### üìä Log Analysis Patterns

**Request Flow Analysis:**
```bash
# Trace complete request lifecycle
grep "Processing chat completion request" logs/server.log | tail -1 | jq '.data.complete_request' | cat

# Analyze vendor selection
grep "Selected vendor" logs/server.log | tail -5 | jq '.data' | cat

# Review response processing
grep "Response processing completed" logs/server.log | tail -1 | jq '.data.complete_response_data' | cat
```

## Development Workflow

> **Prerequisites**: Complete setup instructions are in [Running and Testing Guide](mdc:running_and_testing.mdc)

### Core Development Cycle

```bash
# 1. Make code changes
# 2. Run tests and linting
make test && make lint

# 3. Build and test locally  
make build && ./build/server > logs/server.log 2>&1 & sleep 3 && curl http://localhost:8082/health | cat

# 4. Check logs for issues
tail -20 logs/server.log | grep -E "(error|Error|failed|Failed)" | cat

# 5. Commit when ready (see Git Workflow section)
```

### Sequential Thinking Approach

When approaching complex development tasks, use this systematic methodology:

1. **Analysis Phase**:
   ```bash
   # Research existing patterns
   grep -r "similar_pattern" internal/ | cat
   
   # Understand current architecture
   find internal/ -name "*.go" -exec grep -l "relevant_interface" {} \; | cat
   ```

2. **Planning Phase**:
   - Break down the request into discrete steps
   - Identify all affected components
   - Plan testing approach
   - Consider edge cases and error scenarios

3. **Implementation Phase**:
   - Implement one component at a time
   - Test each component individually
   - Verify logs show expected behavior
   - Check for integration issues

4. **Verification Phase**:
   ```bash
   # Run comprehensive checks
   make ci-check
   
   # Verify logs
   tail -20 logs/server.log | cat
   
   # Test functionality
   curl -X POST http://localhost:8082/v1/chat/completions -H "Content-Type: application/json" -d '{"model":"test","messages":[{"role":"user","content":"test"}]}' | jq | cat
   ```

## AWS Deployment & Monitoring

### Enhanced ECS Deployment

The project includes comprehensive AWS ECS deployment with monitoring capabilities:

**Deployment Prerequisites:**
```bash
# Verify AWS configuration
aws --profile $AWS_PROFILE sts get-caller-identity | cat

# Check ECS cluster status
aws --profile $AWS_PROFILE --region $AWS_REGION ecs describe-clusters --clusters $AWS_CLUSTER_DEV | cat

# Verify ECR repository access
aws --profile $AWS_PROFILE --region $AWS_REGION ecr describe-repositories --repository-names generative-api-router | cat
```

**Deployment Process:**
```bash
# 1. Build and push Docker image
./scripts/deploy.sh

# 2. Monitor deployment
aws --profile $AWS_PROFILE --region $AWS_REGION ecs describe-services --cluster $AWS_CLUSTER_DEV --services $AWS_SERVICE_DEV --query 'services[0].deployments' | cat

# 3. Check task health
aws --profile $AWS_PROFILE --region $AWS_REGION ecs list-tasks --cluster $AWS_CLUSTER_DEV --service-name $AWS_SERVICE_DEV | cat

# 4. Monitor logs
aws --profile $AWS_PROFILE --region $AWS_REGION logs tail /ecs/generative-api-router --follow | cat
```

**Post-Deployment Verification:**
```bash
# Health check
curl -X GET https://your-load-balancer-url/health | cat

# Functional test
curl -X POST https://your-load-balancer-url/v1/chat/completions -H "Content-Type: application/json" -d '{"model":"test","messages":[{"role":"user","content":"Hello"}]}' | jq | cat

# Monitor CloudWatch metrics
aws --profile $AWS_PROFILE --region $AWS_REGION cloudwatch get-metric-statistics --namespace AWS/ECS --metric-name CPUUtilization --dimensions Name=ServiceName,Value=$AWS_SERVICE_DEV --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) --end-time $(date -u +%Y-%m-%dT%H:%M:%S) --period 300 --statistics Average | cat
```

**Troubleshooting Commands:**
```bash
# Check service events
aws --profile $AWS_PROFILE --region $AWS_REGION ecs describe-services --cluster $AWS_CLUSTER_DEV --services $AWS_SERVICE_DEV --query 'services[0].events' | cat

# Get task definition details
aws --profile $AWS_PROFILE --region $AWS_REGION ecs describe-task-definition --task-definition $AWS_SERVICE_DEV --query 'taskDefinition.containerDefinitions[0]' | cat

# Check load balancer health
aws --profile $AWS_PROFILE --region $AWS_REGION elbv2 describe-target-health --target-group-arn your-target-group-arn | cat
```

### CloudWatch Integration

**Log Analysis:**
```bash
# Stream CloudWatch logs
aws --profile $AWS_PROFILE --region $AWS_REGION logs tail /ecs/generative-api-router --follow --filter-pattern "ERROR" | cat

# Query specific time range
aws --profile $AWS_PROFILE --region $AWS_REGION logs filter-log-events --log-group-name /ecs/generative-api-router --start-time $(date -d '1 hour ago' +%s)000 --filter-pattern "ProxyHandler" | cat

# Export logs for analysis
aws --profile $AWS_PROFILE --region $AWS_REGION logs create-export-task --log-group-name /ecs/generative-api-router --from $(date -d '1 day ago' +%s)000 --to $(date +%s)000 --destination your-s3-bucket | cat
```

## Maintenance

*   **Dependencies**: Run `go mod tidy` periodically to clean up dependencies. Key dependencies include:
    *   `github.com/stretchr/testify` v1.10.0 - Testing assertions
    *   `github.com/go-playground/validator/v10` v10.26.0 - Struct validation
*   **Configuration Updates**: API keys (`configs/credentials.json`) and model lists (`configs/models.json`) can be updated without code changes. Restart the server for changes to take effect.
*   **Logging**: Review logs in the `logs/` directory or console output. The server logs selected vendors/models for each request, the original requested model, and how the response model is being presented. Key log lines for debugging model handling include `VERBOSE_DEBUG: ProxyRequest - Original requested model:`, `VERBOSE_DEBUG: ProxyRequest - Selected Vendor:`, and `Processing response from actual model: ... will be presented as:`.
*   **Profiling**: The service includes pprof endpoints for performance profiling at `/debug/pprof/`.
*   **Code Quality**: 
    ```bash
    # Format code
    make format
    
    # Run linter
    make lint
    
    # Clean build artifacts and logs
    make clean
    make clean-logs
    ```

## Common Pitfalls & Lessons Learned

*   **Port Conflicts**: Always ensure port `:8082` (or the configured port) is free before starting the server. Use `sudo lsof -i :<port> | cat` and `sudo kill -9 <PID>`. For the local server, prefer `pgrep -f "^./server$" | xargs kill -9`.
*   **Server Startup Time**: *Always* add a short delay (e.g., `sleep 3`) after starting the server in the background before sending test `curl` requests. This prevents premature "Connection refused" errors.
*   **Curl Command Formatting**: Ensure JSON payloads in `curl -d` arguments are properly quoted and escaped, especially when using multi-line JSON. Single-line JSON with escaped quotes (e.g., `\'{\"key\": \"value\"}\'`) is often safer in scripts.
*   **Background Processes**: When starting the server with `&`, ensure it's managed correctly (e.g., `pgrep -f "^./server$" | xargs kill -9` to stop it specifically).
*   **Error Message Reliance**: Avoid relying on exact string matching for error messages. Use typed errors (`errors.Is()`) for more robust error handling.
*   **Project Structure Changes**: When reorganizing project structure:
    *   Update all file references in code
    *   Update Docker build contexts and COPY commands
    *   Update documentation paths
    *   Run verification scripts to ensure completeness
*   **Security Scans**: Always scan for sensitive data before committing:
    *   AWS account IDs, access keys, secrets
    *   Infrastructure IDs (VPCs, subnets, security groups)
    *   Any hardcoded credentials or tokens

---

Important: you can't read the `configs/credentials.json` using `read_file` tool, you can use `run_terminal_cmd` to `cat configs/credentials.json` instead

## Security Best Practices

### Sensitive Data Management
1. **ALWAYS check for sensitive data before committing**:
   ```bash
   # Check for AWS account IDs (12-digit numbers)
   grep -r -E '\b[0-9]{12}\b' --exclude-dir={.git,node_modules,vendor,.terraform,build} --exclude="*.log" .
   
   # Check for AWS access keys
   grep -r -E '(AKIA|ASIA|aws_access_key|aws_secret|AWS_ACCESS|AWS_SECRET)' --exclude-dir={.git,node_modules,vendor,.terraform,build} --exclude="*.log" .
   ```

2. **Deployment scripts often contain sensitive information**:
   - `scripts/deploy.sh` is gitignored as it contains AWS account IDs, VPC IDs, etc.
   - Never commit files with hardcoded credentials or infrastructure IDs

3. **Use environment variables or secret management for production**

## Git Workflow

### Creating Pull Requests
1. **Create a feature branch**:
   ```bash
   git checkout -b feat/your-feature-name
   ```

2. **Commit with detailed messages using multiple -m flags**:
   ```bash
   git commit -m "feat: main commit message" \
     -m "- First bullet point" \
     -m "- Second bullet point" \
     -m "" \
     -m "- Additional details"
   ```

3. **Push and create PR using GitHub CLI**:
   ```bash
   # Push branch
   git push -u origin feat/your-feature-name
   
   # Create PR with temporary file
   cat > pr-body-temp.md << 'EOF'
   # PR Title
   
   ## Summary
   ...
   EOF
   
   gh pr create --title "feat: your feature" --body-file pr-body-temp.md --base main
   rm pr-body-temp.md
   ```

### Monitoring GitHub Actions & CI/CD

**IMPORTANT**: After creating a PR, you MUST proactively monitor the GitHub Actions CI/CD pipeline and fix any issues before requesting review.

1. **Monitor PR Checks**:
   ```bash
   # Wait for CI to start (usually 30 seconds)
   sleep 30
   
   # Check PR CI status
   gh pr checks <PR_NUMBER>
   
   # Watch checks in real-time (refreshes every 10 seconds)
   watch -n 10 'gh pr checks <PR_NUMBER>'
   ```

2. **View Detailed CI Run**:
   ```bash
   # List recent workflow runs for your branch
   gh run list --branch <your-branch-name> --limit 5
   
   # View specific run details
   gh run view <RUN_ID>
   
   # View failed job logs
   gh run view <RUN_ID> --log-failed
   
   # View specific job logs
   gh run view --job=<JOB_ID>
   ```

3. **Common CI Monitoring Commands**:
   ```bash
   # Check PR status including CI checks
   gh pr view <PR_NUMBER>
   
   # Get PR checks in JSON format
   gh pr checks <PR_NUMBER> --json name,status,conclusion
   
   # Wait for checks to complete
   gh pr checks <PR_NUMBER> --watch
   ```

### Handling CI Failures

If CI checks fail, follow these steps:

1. **Identify the Issue**:
   ```bash
   # View failed logs
   gh run view <RUN_ID> --log-failed | grep -B 10 -A 10 "error\|Error\|failed\|Failed"
   
   # Check specific job that failed
   gh run view <RUN_ID> --job=<JOB_ID>
   ```

2. **Fix Issues Locally**:
   ```bash
   # Run CI checks locally before pushing
   make ci-check
   
   # Individual checks
   make format-check    # Code formatting
   make lint           # Linting
   make test-coverage  # Tests with coverage
   make build          # Build verification
   make security-scan  # Security scanning
   ```

3. **Push Fixes**:
   ```bash
   # After fixing issues
   git add .
   git commit -m "fix: address CI failures" \
     -m "- Fix formatting issues" \
     -m "- Resolve linting errors"
   git push
   
   # Monitor new CI run
   sleep 30 && gh pr checks <PR_NUMBER>
   ```

### Branch Protection Rules

This repository enforces the following rules:
- **No direct pushes to main**: All changes must go through pull requests
- **CI must pass**: PRs cannot be merged unless all CI checks are green
- **Up-to-date branch**: PRs must be up-to-date with main before merging

### PR Lifecycle Best Practices

1. **Before Creating PR**:
   - Run `make ci-check` locally
   - Ensure all tests pass
   - Check for sensitive data leaks

2. **After Creating PR**:
   - Monitor CI pipeline immediately
   - Fix any failures proactively
   - Don't wait for reviewers if CI is failing

3. **Example Complete Workflow**:
   ```bash
   # Create feature branch
   git checkout -b feat/awesome-feature
   
   # Make changes and test locally
   make ci-check
   
   # Commit and push
   git add .
   git commit -m "feat: add awesome feature" -m "- Implementation details"
   git push -u origin feat/awesome-feature
   
   # Create PR
   gh pr create --title "feat: awesome feature" --body "Add awesome feature with XYZ functionality"
   
   # Monitor CI (get PR number from create output)
   PR_NUM=$(gh pr list --head feat/awesome-feature --json number -q '.[0].number')
   echo "Monitoring PR #$PR_NUM"
   
   # Wait and check
   sleep 30
   gh pr checks $PR_NUM --watch
   
   # If failures occur
   gh run list --branch feat/awesome-feature --limit 1
   gh run view <RUN_ID> --log-failed
   
   # Fix and push
   # ... make fixes ...
   git add . && git commit -m "fix: address CI issues" && git push
   
   # Monitor again
   gh pr checks $PR_NUM --watch
   ```

### Core Architecture Principle

**Key Principle**: This service acts as a *transparent proxy* with a key modification. The router selects an actual vendor/model based on its internal logic (e.g., random selection, vendor filter). The `model` field in the request to the downstream vendor is overridden with this *actual selected model*. However, the `model` field in the final response back to the client is modified to reflect the *original model name the client sent in its initial request*. All other request and response data (headers, body structure excluding the model field, status code) from the vendor **must** be passed *exactly* to the client, and vice-versa. Changes must not interfere with this.

### Testing

> **Complete Testing Guide**: For comprehensive testing instructions including unit tests, manual testing, and debugging, see [Running and Testing Guide](mdc:running_and_testing.mdc)

## Release Process

### üè∑Ô∏è MANDATORY Release Workflow

Follow this systematic approach for creating releases:

#### 1. Pre-Commit Review Process

```bash
# 1. List modified files
git status --porcelain | grep '^ M' | cut -c 4- | cat

# 2. Review each file (replace <file> with actual path)
git diff --color <file> | cat

# 3. Check final logs for any issues
tail -10 logs/server.log | cat

# 4. Stage and commit with proper message
git add .
git commit -m "feat: describe changes based on diff review" -m "Detailed description if needed" | cat
```

#### 2. Version Management

```bash
# 1. Get current version and prepare for bump
CURRENT_VERSION=$(git describe --tags --abbrev=0 2>/dev/null || echo "v0.0.0") && echo "Current version: $CURRENT_VERSION" | cat

# 2. Review changes since last release
git log $(git describe --tags --abbrev=0 2>/dev/null || echo "HEAD~10")..HEAD --oneline | cat

# 3. Determine version bump type:
# - patch: Bug fixes, small improvements
# - minor: New features, non-breaking changes  
# - major: Breaking changes, API changes

# 4. Create new version tag (replace X.X.X with new version)
NEW_VERSION="v2.1.0"  # Example: increment based on changes
```

#### 3. Create Release Tag

```bash
# Create annotated tag with detailed release notes
git tag -a $NEW_VERSION -m "$NEW_VERSION: Brief release title" \
  -m "" \
  -m "FEATURES:" \
  -m "- New feature descriptions" \
  -m "" \
  -m "IMPROVEMENTS:" \
  -m "- Performance improvements" \
  -m "- Code quality enhancements" \
  -m "" \
  -m "FIXES:" \
  -m "- Bug fix descriptions" \
  -m "" \
  -m "BREAKING CHANGES:" \
  -m "- Any breaking changes (if major version)" | cat
```

#### 4. Push and Create GitHub Release

```bash
# 1. Push the tag to remote
git push origin $NEW_VERSION | cat

# 2. Create release notes file (temp file for GitHub release)
cat > temp-release-notes.md << 'EOF'
## ‚ú® What's New

### Features
- New feature descriptions with details
- Enhanced functionality explanations

### Improvements  
- Performance optimizations
- Code quality enhancements
- Better error handling

### Bug Fixes
- Fixed specific issues
- Resolved edge cases

### Technical Changes
- Updated dependencies
- Improved test coverage
- Enhanced documentation

## üîß Migration Guide

If there are breaking changes, provide migration instructions here.

## üìä Impact

- Performance improvements
- Reduced memory usage
- Better reliability

## üß™ Testing

- ‚úÖ All tests passing
- ‚úÖ Manual testing completed
- ‚úÖ Performance verified
EOF

# 3. Create and publish GitHub release
gh release create $NEW_VERSION --title "üöÄ Generative API Router $NEW_VERSION" --notes-file temp-release-notes.md | cat

# 4. Cleanup temp files
rm temp-release-notes.md && echo "Cleaned up release notes file" | cat
```

#### 5. Post-Release Verification

```bash
# 1. Verify release was created
gh release view $NEW_VERSION | cat

# 2. Check latest releases
gh release list --limit 3 | cat

# 3. Verify tag exists locally and remotely
git tag --sort=-version:refname | head -5 | cat
```

### ‚úÖ Good Commit Message Examples

```bash
# Feature commits
git commit -m "feat: add health check monitoring" -m "Added comprehensive health check with dependency verification" | cat

# Documentation commits  
git commit -m "docs: update API documentation" -m "Added examples for streaming and tool calling endpoints" | cat

# Fix commits
git commit -m "fix: resolve proxy timeout issues" -m "Increased timeout values and added retry logic for vendor APIs" | cat

# Breaking change commits
git commit -m "feat!: remove metrics implementation" -m "BREAKING CHANGE: /metrics endpoint no longer available" | cat
```

### üìã Pre-Release Checklist

- [ ] All modified files reviewed (`git diff --color <file>`)
- [ ] Logs checked for errors (`tail -20 logs/server.log`)
- [ ] Tests passing (`make test`)
- [ ] Linting clean (`make lint`)
- [ ] Security scan clean (`make security-scan`)
- [ ] Commit message follows conventional format
- [ ] Version bump appropriate for changes
- [ ] Release notes comprehensive and accurate